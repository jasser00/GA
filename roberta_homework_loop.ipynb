{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a66b5cf3",
   "metadata": {},
   "source": [
    "\n",
    "# üîÅ RoBERTa (manual loop) for Sentiment Analysis ‚Äî Google Colab\n",
    "\n",
    "This notebook mirrors the **Part 2 (BERT)** lab style (manual training/validation loops) but uses **RoBERTa** instead of BERT.\n",
    "It includes:\n",
    "- A short **intro to RoBERTa** and **metric definitions** (Accuracy, Precision, Recall, F1)\n",
    "- **IMDB 50K** loading/cleaning (via Kaggle `kagglehub`)\n",
    "- **Tokenization** with `RobertaTokenizerFast`\n",
    "- **Manual training loop** (AdamW + linear scheduler)\n",
    "- **Evaluation** (accuracy/precision/recall/F1 + classification report)\n",
    "- **Comparison table** vs your **BERT** results (paste your BERT scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861083ac",
   "metadata": {},
   "source": [
    "\n",
    "## 1) What is RoBERTa (high level)?\n",
    "**RoBERTa (Robustly Optimized BERT Approach)** keeps BERT's encoder architecture but changes the pre-training recipe:\n",
    "- More data and longer training\n",
    "- **Dynamic masking**\n",
    "- Drops **Next Sentence Prediction**\n",
    "- Larger batches and tuned hyperparameters\n",
    "**Key takeaway:** same architecture as BERT, **stronger pre-training** ‚Üí often **better downstream performance**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc247ef",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Metrics ‚Äî Definitions & When to Use\n",
    "\n",
    "Binary classification with confusion counts **TP, FP, FN, TN**.\n",
    "\n",
    "\\[\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "\\text{F1} = 2 \\times \\frac{\\text{Precision}\\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "\\]\n",
    "\n",
    "| Metric | Use when | Why |\n",
    "|---|---|---|\n",
    "| Accuracy | Classes roughly balanced | Global performance |\n",
    "| Precision | False positives are costly | Fewer negatives mislabeled positive |\n",
    "| Recall | False negatives are costly | Catch more positives |\n",
    "| F1 | Need balance of P/R | Single balanced score |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6a1672",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Setup (Colab)\n",
    "Run the following cell to install requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd2e03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip -q install transformers datasets scikit-learn kagglehub accelerate -U\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da06e0e1",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Load & Clean IMDB 50K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebe5924",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "import kagglehub, shutil, os\n",
    "\n",
    "# Download the dataset from Kaggle with kagglehub\n",
    "path = kagglehub.dataset_download(\"lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\")\n",
    "print(\"Dataset downloaded to:\", path)\n",
    "\n",
    "# Copy to Drive for persistence (optional)\n",
    "drive_path = '/content/drive/MyDrive/KaggleDatasets/IMDB_50K/'\n",
    "os.makedirs(drive_path, exist_ok=True)\n",
    "shutil.copytree(path, drive_path, dirs_exist_ok=True)\n",
    "print(\"Dataset copied to Google Drive at:\", drive_path)\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(os.path.join(drive_path, 'IMDB Dataset.csv'))\n",
    "print(\"Raw shape:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e7706d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Clean reviews and encode labels\n",
    "df['review_cleaned'] = (\n",
    "    df['review']\n",
    "      .str.replace('<br />', ' ', regex=False)\n",
    "      .str.replace('\\s+', ' ', regex=True)\n",
    "      .str.strip()\n",
    ")\n",
    "df['label'] = df['sentiment'].map({'negative': 0, 'positive': 1}).astype(int)\n",
    "df[['review_cleaned','sentiment','label']].head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e3e77e",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Train/Validation Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f84e5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['review_cleaned'].tolist(),\n",
    "    df['label'].tolist(),\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df['label'].tolist()\n",
    ")\n",
    "\n",
    "len(train_texts), len(val_texts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9da914f",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Tokenization (RoBERTa) ‚Üí Tensors\n",
    "We use `RobertaTokenizerFast` with `max_length=128` for faster training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a279347",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "MAX_LEN = 128\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
    "\n",
    "def encode_texts(texts):\n",
    "    enc = tokenizer(\n",
    "        texts,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return enc['input_ids'], enc['attention_mask']\n",
    "\n",
    "train_input_ids, train_attention = encode_texts(train_texts)\n",
    "val_input_ids,   val_attention   = encode_texts(val_texts)\n",
    "\n",
    "train_labels_t = torch.tensor(train_labels)\n",
    "val_labels_t   = torch.tensor(val_labels)\n",
    "\n",
    "train_input_ids.shape, val_input_ids.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93e36e3",
   "metadata": {},
   "source": [
    "\n",
    "## 7) DataLoaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd76b08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_ds = TensorDataset(train_input_ids, train_attention, train_labels_t)\n",
    "val_ds   = TensorDataset(val_input_ids,   val_attention,   val_labels_t)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "len(train_loader), len(val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b159ec81",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Model, Optimizer, Scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910158ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import RobertaForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if device.type == 'cuda':\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "EPOCHS = 2\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "\n",
    "total_steps = EPOCHS * len(train_loader)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def batch_accuracy(logits, labels):\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    return (preds == labels).float().mean().item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca97fbb",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Training & Validation Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6652df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "    # ---- Train ----\n",
    "    model.train()\n",
    "    train_loss, train_acc = 0.0, 0.0\n",
    "    for input_ids, attn, labels in tqdm(train_loader, desc=\"Training\", leave=False):\n",
    "        input_ids = input_ids.to(device)\n",
    "        attn      = attn.to(device)\n",
    "        labels    = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attn, labels=labels, return_dict=True)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_acc  += batch_accuracy(logits.detach(), labels)\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_acc  /= len(train_loader)\n",
    "\n",
    "    # ---- Validate ----\n",
    "    model.eval()\n",
    "    val_loss, val_acc = 0.0, 0.0\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attn, labels in tqdm(val_loader, desc=\"Validating\", leave=False):\n",
    "            input_ids = input_ids.to(device)\n",
    "            attn      = attn.to(device)\n",
    "            labels    = labels.to(device)\n",
    "            outputs = model(input_ids, attention_mask=attn, labels=labels, return_dict=True)\n",
    "            val_loss += outputs.loss.item()\n",
    "            val_acc  += batch_accuracy(outputs.logits, labels)\n",
    "            all_logits.append(outputs.logits.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc  /= len(val_loader)\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "logits = torch.cat(all_logits, dim=0)\n",
    "labels = torch.cat(all_labels, dim=0)\n",
    "probs = F.softmax(logits, dim=1).numpy()\n",
    "preds = np.argmax(probs, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3995e9d",
   "metadata": {},
   "source": [
    "\n",
    "## 10) Metrics (Accuracy, Precision, Recall, F1) + Report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771371fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "y_true = labels.numpy()\n",
    "y_pred = preds\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "prec = precision_score(y_true, y_pred, average='binary')\n",
    "rec = recall_score(y_true, y_pred, average='binary')\n",
    "f1 = f1_score(y_true, y_pred, average='binary')\n",
    "\n",
    "print(f\"RoBERTa ‚Äî Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}\\n\")\n",
    "print(classification_report(y_true, y_pred, target_names=['negative','positive']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238961bb",
   "metadata": {},
   "source": [
    "\n",
    "## 11) Overview Comparison Table (Paste your BERT metrics)\n",
    "Enter your **BERT (base-uncased)** metrics from Lab Part 2, and we'll build a side-by-side table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b908c656",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# === ENTER YOUR BERT METRICS HERE (percentages) ===\n",
    "BERT_accuracy   = 91.8\n",
    "BERT_precision  = 91.8\n",
    "BERT_recall     = 91.8\n",
    "BERT_f1         = 91.8\n",
    "\n",
    "overview = pd.DataFrame({\n",
    "    'Model': ['BERT (base-uncased)', 'RoBERTa (base)'],\n",
    "    'Accuracy':  [BERT_accuracy,  round(acc*100, 1)],\n",
    "    'Precision': [BERT_precision, round(prec*100,1)],\n",
    "    'Recall':    [BERT_recall,    round(rec*100, 1)],\n",
    "    'F1':        [BERT_f1,        round(f1*100,  1)],\n",
    "})\n",
    "overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68336bb2",
   "metadata": {},
   "source": [
    "\n",
    "## 12) Save Outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4a4f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "csv_path = '/content/roberta_vs_bert_overview_loop.csv'\n",
    "overview.to_csv(csv_path, index=False)\n",
    "print(f\"Saved comparison CSV to: {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7954c14",
   "metadata": {},
   "source": [
    "\n",
    "## 13) Quick Inference on Custom Sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389e7c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_sentences = [\n",
    "    \"I love this movie! It was fantastic.\",\n",
    "    \"The product broke after one use, terrible experience.\",\n",
    "    \"Not bad, but could be better.\"\n",
    "]\n",
    "\n",
    "enc = tokenizer(\n",
    "    test_sentences,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    max_length=MAX_LEN,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(enc['input_ids'].to(device), attention_mask=enc['attention_mask'].to(device), return_dict=True)\n",
    "    p = F.softmax(out.logits, dim=1).cpu().numpy()\n",
    "    pred = p.argmax(axis=1)\n",
    "\n",
    "for s, pp, pr in zip(test_sentences, p, pred):\n",
    "    print(f\"Text: {s}\\nProb [neg,pos]: {pp} -> Pred: {['negative','positive'][pr]}\\n\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
